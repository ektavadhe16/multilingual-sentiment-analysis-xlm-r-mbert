{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT-lG4KdPL48"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('insta_data.csv')\n"
      ],
      "metadata": {
        "id": "CgZ8HBZmfeYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"@\\w+|#\", '', text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "    text = text.strip().lower()\n",
        "    return text\n",
        "\n",
        "df['Post Description'] = df['Post Description'].astype(str).apply(clean_text)\n",
        "\n",
        "label_map = {\n",
        "    'negative': 0,\n",
        "    'neutral': 1,\n",
        "    'positive': 2\n",
        "}\n",
        "\n",
        "\n",
        "df['Sentiment'] = df['Sentiment'].astype(str).str.strip().str.lower().map(label_map)\n",
        "\n",
        "df = df.dropna(subset=['Sentiment'])\n",
        "df['Sentiment'] = df['Sentiment'].astype(int)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "YiBxPf2jQU-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentiment_counts = df['Sentiment'].value_counts().sort_index()\n",
        "label_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=label_names, y=sentiment_counts.values, palette='Set2')\n",
        "plt.title('Sentiment Distribution - Bar Chart')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kfSpPNSkQVBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pie(sentiment_counts.values, labels=label_names, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set3'))\n",
        "plt.title('Sentiment Distribution - Pie Chart')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vVmuIRm8QVEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = df['Post Description'].values\n",
        "labels = df['Sentiment'].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(X_train)}\")\n",
        "print(f\"Test size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "jvfq-holQVGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets -q\n"
      ],
      "metadata": {
        "id": "aTWRxhFBQVJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        "\n",
        "    )\n"
      ],
      "metadata": {
        "id": "BGlCG1xDQVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class InstaSentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "train_dataset = InstaSentimentDataset(train_encodings, y_train)\n",
        "test_dataset = InstaSentimentDataset(test_encodings, y_test)\n"
      ],
      "metadata": {
        "id": "orfqNP1LQVPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "wUmGPpyuQVR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds) * 100\n",
        "    f1 = f1_score(labels, preds, average='macro') * 100\n",
        "\n",
        "    return {\n",
        "        'accuracy': round(acc, 2),\n",
        "        'f1': round(f1, 2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5yxW40h0Qv2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "5YGV6tFIQv5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=3) # 3 for negative, neutral, positive\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "7e8LMr_FQv71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "gS_6QgFdQwDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\n Final Test Accuracy: {eval_results['eval_accuracy']:.2f}%\")\n",
        "print(f\" Final F1 Score (Macro): {eval_results['eval_f1']:.2f}%\")\n"
      ],
      "metadata": {
        "id": "yoH6wbhFQ-41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = predictions.predictions.argmax(-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "Y0IURmkPERwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = pd.DataFrame(trainer.state.log_history)\n",
        "logs = logs.dropna(subset=['eval_accuracy'])\n",
        "\n",
        "plt.plot(logs['epoch'], logs['eval_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(logs['epoch'], logs['eval_f1'], label='Validation F1')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Metric\")\n",
        "plt.title(\"Accuracy & F1 over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iSqeenj1td-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs['error_rate'] = 1 - logs['eval_accuracy']\n",
        "\n",
        "plt.plot(logs['epoch'], logs['error_rate'], label='Validation Error Rate', color='red')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.title(\"Error Rate vs Epoch\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_xAH2cLqtpcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RJq_4WCmtuja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "y_test_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
        "y_score = predictions.predictions\n",
        "\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(3):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "for i, label in enumerate(['Negative', 'Neutral', 'Positive']):\n",
        "    plt.plot(fpr[i], tpr[i], label=f'{label} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title('ROC Curve (One-vs-Rest)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "99924HhWtwvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "sensitivity = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "specificity = []\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "for i in range(3):\n",
        "    tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
        "    fp = cm[:, i].sum() - cm[i, i]\n",
        "    specificity.append(tn / (tn + fp))\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average=None)\n",
        "fmajor = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "for i, cls in enumerate(['Negative', 'Neutral', 'Positive']):\n",
        "    print(f\"\\nClass: {cls}\")\n",
        "    print(f\"  Precision   : {precision[i]:.2f}\")\n",
        "    print(f\"  Sensitivity : {sensitivity[i]:.2f}\")\n",
        "    print(f\"  Specificity : {specificity[i]:.2f}\")\n",
        "\n",
        "print(f\"\\nFmajor (Macro F1): {fmajor:.2f}\")\n"
      ],
      "metadata": {
        "id": "GOqLCYsot1ik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}